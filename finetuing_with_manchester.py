# -*- coding: utf-8 -*-
"""finetuing_with_manchester.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W2GVXtAX6tiOEoxbUrf9Bf5fwTLXs1Wf
"""

import subprocess

# download data
subprocess.run(['wget', 'https://drive.google.com/uc?export=download&id=10cgNV1asbeOf4VPZ9VFH9ngqRjHHQigr', '-O', 'test.csv'])
subprocess.run(['wget', 'https://drive.google.com/uc?export=download&id=1oT5MZBbrzcewwg4yknFemVN79OURWC28', '-O', 'train.csv'])
subprocess.run(['wget', 'https://drive.google.com/uc?export=download&id=1buunGbwqOhZJ_PxjU6D3EyflufFqA9J6', '-O', 'valid.csv'])


# download the pretrained model
subprocess.run(['wget', 'https://drive.google.com/uc?export=download&id=1MfchARlNL5WLIY4lFiUNw1INxJJnvZMP', '-O', 'thomas-pretrained-model-final-single-context'])



from transformers import AutoTokenizer, BartForConditionalGeneration
model_ckpt="./thomas-pretrained-model-final-single-context"
device="cuda"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = BartForConditionalGeneration.from_pretrained(model_ckpt).to(device)

import pandas as pd
from datasets import Dataset, DatasetDict
df_train = pd.read_csv("train.csv")
df_valid = pd.read_csv("valid.csv")
df_test = pd.read_csv("test.csv")

df_train = df_train.dropna()
df_valid = df_valid.dropna()
df_test = df_test.dropna()

ds_train = Dataset.from_pandas(df_train)
ds_valid = Dataset.from_pandas(df_valid)
ds_test = Dataset.from_pandas(df_test)


text = ds_train[:]["input"]
text.extend(ds_train[:]["output"])
text.extend(ds_valid[:]["input"])
text.extend(ds_valid[:]["output"])
text.extend(ds_test[:]["input"])
text.extend(ds_test[:]["output"])

tokenset = list(set(str.split(' '.join(text))))
print(len(tokenizer))  # 57100
tokenizer.add_tokens(tokenset)
print(len(tokenizer))  # 60688

model.resize_token_embeddings(len(tokenizer))

def convert_examples_to_features(example_batch):
   input_encodings = tokenizer(example_batch["input"], max_length=1024,
                               truncation=True)

   with tokenizer.as_target_tokenizer():
       target_encodings = tokenizer(example_batch["output"], max_length=1024,
                                    truncation=True)

   return {"input_ids": input_encodings["input_ids"],
           "attention_mask": input_encodings["attention_mask"],
           "labels": target_encodings["input_ids"]}

dataset_pt_train = ds_train.map(convert_examples_to_features, batched=True)
dataset_pt_train = dataset_pt_train.shuffle(seed=42)
dataset_pt_valid = ds_valid.map(convert_examples_to_features, batched=True)

columns = ["input_ids", "labels", "attention_mask"]

dataset_pt_train.set_format(type="torch", columns=columns)
dataset_pt_valid.set_format(type="torch", columns=columns)

# early stopping
from transformers import EarlyStoppingCallback
early_stopping_callback = EarlyStoppingCallback(
    early_stopping_patience=3)

from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer

seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

training_args = TrainingArguments(
   output_dir='finetuned-manchester', num_train_epochs=10, warmup_steps=500,
   per_device_train_batch_size=32, per_device_eval_batch_size=64,
   weight_decay=0.01, logging_steps=50, push_to_hub=False,
   eval_steps=658, save_steps=658000,gradient_accumulation_steps=4,
   metric_for_best_model="eval_loss",save_strategy='steps',eval_strategy="steps",
   load_best_model_at_end=True)

trainer = Trainer(model=model, args=training_args,
                 tokenizer=tokenizer, data_collator=seq2seq_data_collator,
                 train_dataset=dataset_pt_train,
                 eval_dataset=dataset_pt_valid,
                 callbacks=[early_stopping_callback])


import wandb
from huggingface_hub import notebook_login
import torch

notebook_login()
wandb.init(mode="disabled")

# hide_output
torch.cuda.empty_cache()
trainer.train()
# To save your fine-tuned model:
trainer.save_model("finetuned-model-manchester")
