# -*- coding: utf-8 -*-
"""Pretrain_with_Thomas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h68yGQAlGMnlpmHKwHa7kyZBLnXfbOeu
"""
import subprocess

# download the data
subprocess.run(['wget', 'https://drive.google.com/uc?export=download&id=1sqEeUje554WR0uljkmYl1iEUX38mgo6N', '-O', 'Section_A_pairs_thomas_final_single_context.csv'])


import pandas as pd
df = pd.read_csv("Section_A_pairs_thomas_final_single_context.csv")
df = df.dropna()

# divide data into train and valid
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict

ds=Dataset.from_pandas(df)
ds=ds.train_test_split(test_size=0.1,seed=99)

ds = DatasetDict({
'train': ds['train'],
'valid': ds['test']})

text=ds["train"][:]["context"]
text.extend(ds["valid"][:]["context"])
text.extend(ds["train"][:]["response"])
text.extend(ds["valid"][:]["response"])

from transformers import AutoTokenizer, BartForConditionalGeneration
device="cuda"
tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-base").to(device)

tokenset = list(set(str.split(' '.join(text))))
print(len(tokenizer))  # 50265
tokenizer.add_tokens(tokenset)
print(len(tokenizer))  # 57005

model.resize_token_embeddings(len(tokenizer))


def convert_examples_to_features(example_batch):
   input_encodings = tokenizer(example_batch["context"], max_length=1024,
                               truncation=True)

   with tokenizer.as_target_tokenizer():
       target_encodings = tokenizer(example_batch["response"], max_length=1024,
                                    truncation=True)

   return {"input_ids": input_encodings["input_ids"],
           "attention_mask": input_encodings["attention_mask"],
           "labels": target_encodings["input_ids"]}

dataset_pt = ds.map(convert_examples_to_features,
                                      batched=True)
columns = ["input_ids", "labels", "attention_mask"]
dataset_pt.set_format(type="torch", columns=columns)

from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer
from transformers import EarlyStoppingCallback

early_stopping_callback = EarlyStoppingCallback(
    early_stopping_patience=3,  # 当验证集结果没有改善时，等待6个epoch再停止
)

seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

training_args = TrainingArguments(
   output_dir='Thomas-pretrain-single-context', num_train_epochs=30, warmup_steps=500,
   per_device_train_batch_size=32, per_device_eval_batch_size=64,
   weight_decay=0.01, logging_steps=50, push_to_hub=False,eval_steps=1000,
   save_steps=1e6,gradient_accumulation_steps=4,
   metric_for_best_model="eval_loss",save_strategy='steps',eval_strategy="steps",
   load_best_model_at_end=True)

trainer = Trainer(model=model, args=training_args,
                 tokenizer=tokenizer, data_collator=seq2seq_data_collator,
                 train_dataset=dataset_pt["train"],
                 eval_dataset=dataset_pt["valid"],
                 callbacks=[early_stopping_callback])


import wandb
from huggingface_hub import notebook_login
import torch

notebook_login()
wandb.init(mode="disabled")

# hide_output
torch.cuda.empty_cache()
trainer.train()
# To save the model:
trainer.save_model("thomas-pretrained-model-final-single-context")
