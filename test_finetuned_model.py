# -*- coding: utf-8 -*-
"""test-finetuned-model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1osf2_DH8f00D_0nuEnSV3RobP3eiDw8s
"""

!pip install accelerate -U
!pip install transformers -U
!pip install datasets
!pip install py7zr
!pip install tiktoken
!pip install sentencepiece
!pip install evaluate
!pip install rouge_score

import transformers
from transformers import pipeline, set_seed
import py7zr
import accelerate
import pandas as pd
import torch
import numpy as np

#download splited test sets
!gdown 1w6u_xDl_QLqlCku4QhrF1tjRSkEMsVYu
!unzip test_split.zip

# download finetuned model
!gdown 1rJxVYdZxhJT73dyyYX3NjE4KdZetaBQ_
!unzip finetuned-model-manchester.zip

# download pretrained model
!gdown 1MfchARlNL5WLIY4lFiUNw1INxJJnvZMP
!unzip thomas-pretrained-model-final-single-context.zip

from transformers import AutoTokenizer, BartForConditionalGeneration
model_ckpt="./finetuned-model-manchester"
device="cuda"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = BartForConditionalGeneration.from_pretrained(model_ckpt).to(device)

from transformers import AutoTokenizer, BartForConditionalGeneration
model_ckpt_base="./thomas-pretrained-model-final-single-context"
device="cuda"
tokenizer_base = AutoTokenizer.from_pretrained(model_ckpt_base)
model_base = BartForConditionalGeneration.from_pretrained(model_ckpt_base).to(device)

"""Generation and Evaluation"""

df = pd.read_csv("test_split/Anne_young.csv")
df=df.dropna()
from datasets import Dataset, DatasetDict
ds=Dataset.from_pandas(df)

#finetuned model
torch.cuda.empty_cache()
predictions=[]
for i in range(ds.shape[0]):
  input_ = tokenizer.batch_encode_plus(ds[i:i+1]["input"], max_length=1024,truncation=True,
                                       padding='longest', return_tensors="pt")
  input_ids = input_['input_ids']
  input_mask = input_['attention_mask']
  transformers.set_seed(42)
  responses = model.generate(input_ids=input_ids.to(device),
                         attention_mask=input_mask.to(device),
                         do_sample=True,
                         top_k=100,
                         top_p=0.7,
                         temperature=1.2,
                         num_beams=1,
                         no_repeat_ngram_size=2,
                         early_stopping=False,
                         num_return_sequences=7,
                         max_length=1024
                         )
  predictions.extend(tokenizer.batch_decode(responses, skip_special_tokens=True))
len(predictions)

predictions

# save the predictions as excel
import pandas as pd

references = ds["output"]
num_return_sequences = 7

expanded_references = []
for ref in references:
    expanded_references.extend([ref] * num_return_sequences)


assert len(expanded_references) == len(predictions)

df_long = pd.DataFrame({
    "reference": expanded_references,
    "prediction": predictions
})


df_long.to_excel("predictions_Warren_old.xlsx", index=False)

# pretrained model
torch.cuda.empty_cache()
predictions_base=[]
for i in range(ds.shape[0]):
  input_ = tokenizer_base.batch_encode_plus(ds[i:i+1]["input"], max_length=1024,truncation=True,
                                            padding='longest', return_tensors="pt")
  input_ids = input_['input_ids']
  input_mask = input_['attention_mask']
  transformers.set_seed(42)
  responses_base = model_base.generate(input_ids=input_ids.to(device),
                         attention_mask=input_mask.to(device),
                         do_sample=True,
                         top_k=100,
                         top_p=0.7,
                         temperature=1.2,
                         num_beams=1,
                         no_repeat_ngram_size=2,
                         early_stopping=False,
                         num_return_sequences=7,
                         max_length=1024)
  predictions_base.extend(tokenizer_base.batch_decode(responses_base, skip_special_tokens=True))
len(predictions_base)

predictions_base

grouped_predictions = [predictions[i:i+7] for i in range(0, len(predictions), 7)]
assert len(ds['output'])==len(grouped_predictions)

grouped_predictions_base = [predictions_base[i:i+7] for i in range(0, len(predictions_base), 7)]
assert len(ds['output'])==len(grouped_predictions_base)

"""BLEU"""

import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

# select best bleu for finetuned model
best_predictions = []
references=ds['output']

for i in range(len(references)):
    ref = references[i]
    context_predictions = grouped_predictions[i]

    best_bleu_score = -1
    best_prediction = None

    for pred in context_predictions:
        score = sentence_bleu([ref], pred)
        if score > best_bleu_score:
            best_bleu_score = score
            best_prediction = pred

    best_predictions.append({'reference': ref, 'prediction': best_prediction, 'bleu_score': best_bleu_score})

df_bleu = pd.DataFrame(best_predictions)
ds_bleu = Dataset.from_pandas(df_bleu)
df_bleu

# select best bleu for pretrained model
best_predictions_base = []
references=ds['output']

for i in range(len(references)):
    ref = references[i]
    context_predictions_base = grouped_predictions_base[i]

    best_bleu_score = -1
    best_prediction = None

    for pred_base in context_predictions_base:
        score = sentence_bleu([ref], pred_base)
        if score > best_bleu_score:
            best_bleu_score = score
            best_prediction = pred_base

    best_predictions_base.append({'reference': ref, 'prediction': best_prediction, 'bleu_score': best_bleu_score})

df_bleu_base = pd.DataFrame(best_predictions_base)
ds_bleu_base = Dataset.from_pandas(df_bleu_base)
df_bleu_base

# overall bleu for finetuned model
import evaluate
references  = ds['output']
predictions = ds_bleu['prediction']
# bleu = evaluate.load("bleu")
# bleu.add(predictions=str(predictions), references=references)
# results = bleu.compute()
from evaluate import load
bleu = load("bleu")
results = bleu.compute(predictions=predictions, references=references)
print(results)

# overall bleu for pretrained model
import evaluate
references_base  = ds['output']
predictions_base = ds_bleu_base['prediction']
# bleu_base = evaluate.load("bleu")
# bleu_base.add(predictions=str(predictions_base), references=str(references_base))
# results_base = bleu_base.compute()
from evaluate import load
bleu_base = load("bleu")
results_base = bleu_base.compute(predictions=predictions_base, references=references_base)
print(results_base)

"""ROUGE"""

import evaluate
import pandas as pd
from datasets import Dataset

# selected best rouge sentence for finetuned model
rouge = evaluate.load("rouge")
references = ds['output']

best_predictions = []

for i in range(len(references)):
    reference = references[i]
    candidates = grouped_predictions[i]

    best_rougeL_score = -1
    best_pred = None
    best_metrics = {}

    for pred in candidates:
        results = rouge.compute(predictions=[pred], references=[reference], use_aggregator=False)
        rougeL = results["rougeL"][0]

        if rougeL > best_rougeL_score:
            best_rougeL_score = rougeL
            best_pred = pred
            best_metrics = {
                'rouge1': results['rouge1'][0],
                'rouge2': results['rouge2'][0],
                'rougeL': results['rougeL'][0]
            }

    best_predictions.append({
        'reference': reference,
        'best_prediction': best_pred,
        'rouge1': best_metrics['rouge1'],
        'rouge2': best_metrics['rouge2'],
        'rougeL': best_metrics['rougeL']
    })


df_rouge = pd.DataFrame(best_predictions)
ds_rouge = Dataset.from_pandas(df_rouge)
df_rouge

# select best rouge sentence for pretrained model
rouge = evaluate.load("rouge")
references = ds['output']

best_predictions_base = []

for i in range(len(references)):
    reference = references[i]
    candidates = grouped_predictions_base[i]

    best_rougeL_score = -1
    best_pred = None
    best_metrics = {}

    for pred in candidates:
        results = rouge.compute(predictions=[pred], references=[reference], use_aggregator=False)
        rougeL = results["rougeL"][0]

        if rougeL > best_rougeL_score:
            best_rougeL_score = rougeL
            best_pred = pred
            best_metrics = {
                'rouge1': results['rouge1'][0],
                'rouge2': results['rouge2'][0],
                'rougeL': results['rougeL'][0]
            }

    best_predictions_base.append({
        'reference': reference,
        'best_prediction': best_pred,
        'rouge1': best_metrics['rouge1'],
        'rouge2': best_metrics['rouge2'],
        'rougeL': best_metrics['rougeL']
    })

df_rouge_base = pd.DataFrame(best_predictions_base)
ds_rouge_base = Dataset.from_pandas(df_rouge_base)
df_rouge_base

# overall rouge for finetuned model
import evaluate
references  = ds[:]["output"]
predictions = ds_rouge[:]['best_prediction']

rouge   = evaluate.load("rouge")
results = rouge.compute(predictions=predictions, references=references)

print(results)

# overall rouge for pretrained model
import evaluate
references_base  = ds[:]["output"]
predictions_base = ds_rouge_base[:]['best_prediction']

rouge_base   = evaluate.load("rouge")
results_base = rouge_base.compute(predictions=predictions_base, references=references_base)

print(results_base)

"""Relative PPL"""

# download finetuned model
!gdown 1rJxVYdZxhJT73dyyYX3NjE4KdZetaBQ_
!unzip finetuned-model-manchester.zip

from transformers import AutoTokenizer, BartForConditionalGeneration
model_ckpt="./finetuned-model-manchester"
device="cuda"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = BartForConditionalGeneration.from_pretrained(model_ckpt).to(device)

import torch
import math

import pandas as pd
df = pd.read_csv("Gail_old.csv")
df=df.dropna()
from datasets import Dataset, DatasetDict
ds=Dataset.from_pandas(df)

def compute_perplexity(context, response):
    model.eval()


    input_ids = tokenizer(context, return_tensors="pt", truncation=True, max_length=1024).input_ids.to(device)


    with tokenizer.as_target_tokenizer():
        labels = tokenizer(response, return_tensors="pt", truncation=True, max_length=1024).input_ids.to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss


    return math.exp(loss.item())

import os
import pandas as pd
import torch
import math
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm

input_folder = "/content"
results = []

for filename in os.listdir(input_folder):
    if not filename.endswith(".csv"):
        continue

    file_path = os.path.join(input_folder, filename)
    df = pd.read_csv(file_path).dropna()

    if 'input' not in df.columns or 'output' not in df.columns:
        print(f"Skipped {filename} (missing 'input' or 'output')")
        continue

    total_log_ppl = 0
    count = 0

    for context, response in zip(df['input'], df['output']):
        try:
            ppl = compute_perplexity(context, response)
            total_log_ppl += math.log(ppl)
            count += 1
        except Exception as e:
            print(f"Error in {filename}: {e}")

    if count > 0:
        avg_loss = total_log_ppl / count
        avg_ppl = math.exp(avg_loss)
        print(f"{filename}: {avg_ppl:.2f}")
        results.append({"file_name": filename, "avg_perplexity": avg_ppl})
    else:
        print(f"{filename}: No valid samples")
        results.append({"file_name": filename, "avg_perplexity": None})
df_result = pd.DataFrame(results)

import pandas as pd
df = pd.read_csv("Warren_old.csv")
df=df.dropna()
from datasets import Dataset, DatasetDict
ds=Dataset.from_pandas(df)

total_loss = 0
for context, response in zip(ds['input'], ds['output']):
    ppl = compute_perplexity(context, response)
    total_loss += math.log(ppl)

avg_loss = total_loss / len(ds['input'])
avg_perplexity = math.exp(avg_loss)

print(f"Average Perplexity {avg_perplexity:.2f}")